{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widjets\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from time import time\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "from sklearn import metrics\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'Z:\\test tasks\\Sber_robo\\project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from project.model import my_segmentator\n",
    "model,preprocessing_fn = my_segmentator()\n",
    "model.load_state_dict(torch.load(r'Z:\\test tasks\\Sber_robo\\project\\logs\\checkpoints\\last_full.pth')['model_state_dict'])\n",
    "model.eval().cuda()\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as albu\n",
    "import cv2\n",
    "from albumentations.pytorch import ToTensor\n",
    "\n",
    "BORDER_CONSTANT = 0\n",
    "def pre_transforms(image_size):\n",
    "    return albu.Compose([\n",
    "        albu.LongestMaxSize(max_size=image_size, always_apply=True),\n",
    "        albu.PadIfNeeded(min_height=None, min_width=None, pad_height_divisor=32, pad_width_divisor=32, border_mode=BORDER_CONSTANT, value=0),\n",
    "    ])\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    return albu.Compose([\n",
    "        albu.Lambda(name='my_transform', image=preprocessing_fn),\n",
    "        ToTensor(),\n",
    "    ])\n",
    "\n",
    "valid_transforms = pre_transforms\n",
    "preprocess_tr = get_preprocessing(preprocessing_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 50 images\n"
     ]
    }
   ],
   "source": [
    "from dataset_SDD import SDD_dataset, CLASSES\n",
    "from dataset_UAVid import UAVid_dataset\n",
    "\n",
    "SDD_val_PATH = r'Z:\\test tasks\\Sber_robo\\SDD_images\\imgs\\test'\n",
    "\n",
    "valid_transforms = pre_transforms(image_size=512)\n",
    "SDD_val = SDD_dataset(SDD_val_PATH, augmentations=valid_transforms, preprocessing=None)\n",
    "\n",
    "#############\n",
    "UAVid_val_PATH = r'Z:\\test tasks\\Sber_robo\\uavid_v1.5_official_release_image\\uavid_val'\n",
    "\n",
    "valid_transforms = pre_transforms(image_size=1024)\n",
    "UAVid_val = UAVid_dataset(UAVid_val_PATH, augmentations=valid_transforms, preprocessing=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = {'restricted_area': 0, 'road': 1, 'building': 2, 'human': 3, 'car': 4}\n",
    "CLASSES = CLASSES.keys()\n",
    "DEFAULT_COLORS = ('red', 'blue', 'yellow', 'magenta', 'green', 'indigo', 'darkorange', 'cyan', 'pink', 'yellowgreen')\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "patches = []\n",
    "for i, cls in enumerate(CLASSES):\n",
    "    patches.append(mpatches.Patch(color=DEFAULT_COLORS[i], label=cls))\n",
    "\n",
    "def mask_to_overlay_image(image, labels, mask_strength=0.5):\n",
    "    from skimage.color import label2rgb\n",
    "    \n",
    "    mask = label2rgb(labels, bg_label=-1) \n",
    "\n",
    "    image = np.array(image) / 255.0\n",
    "    image_with_overlay = image * (1 - mask_strength) + mask * mask_strength\n",
    "    image_with_overlay = (\n",
    "        (image_with_overlay * 255).clip(0, 255).round().astype(np.uint8)\n",
    "    )\n",
    "\n",
    "    return image_with_overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python37\\lib\\site-packages\\ipykernel_launcher.py:6: TqdmDeprecationWarning:\n",
      "\n",
      "This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b9542cdb7e94999873272886ab6be66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=60.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "images_paths = Path(r'Q:\\Downloads\\stanford_campus_dataset\\annotations').glob('*/*/reference.jpg')\n",
    "valid_transforms = pre_transforms(image_size=512)\n",
    "Inference_images = []\n",
    "Inference_masks = []\n",
    "with torch.no_grad():\n",
    "    for img_p in tqdm_notebook(list(images_paths)):\n",
    "        img = cv2.imread(str(img_p))[:,:,::-1]\n",
    "        img = valid_transforms(image=img)['image']\n",
    "        \n",
    "        \n",
    "        tensor_img = preprocess_tr(image=img)['image'].unsqueeze(0)\n",
    "        preds = model(tensor_img.cuda())\n",
    "        \n",
    "        Inference_images.append(img)\n",
    "        Inference_masks.append(torch.argmax(preds[0], dim=0).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a77795cea044d3aa24a96ab1237f171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='i', max=59), Output()), _dom_classes=('widget-interact',â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(i=(0,len(Inference_images)-1,1))\n",
    "def tst(i=0):\n",
    "    img = Inference_images[i]\n",
    "    masks = Inference_masks[i]\n",
    "    \n",
    "    plt.figure(figsize=(15,20))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(img)\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    \n",
    "    a = mask_to_overlay_image(img, masks)\n",
    "    plt.legend(handles=patches, ncol=7, bbox_to_anchor=(-0.1, 1),\n",
    "              loc='lower left', fontsize='small')\n",
    "    plt.imshow(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
